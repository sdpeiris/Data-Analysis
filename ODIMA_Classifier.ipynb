{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ODIMA Machine Learning Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###setting environment up\n",
    "%reset -f\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import plotly\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.utils import resample\n",
    "import scipy.stats as stats\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data and creating a combined dataset for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Load your data\n",
    "file_path = 'mODIMA _Neuro_Scores.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "#print(data.head())\n",
    "#type(data)\n",
    "\n",
    "## converting the category column to be binary\n",
    "#data['Category'] = data['Category'].replace({'HC': 0, 'MCI': 1}) ## replace is no longer in use in the future\n",
    "data['Category'] = np.where(data['Category'] == 'HC', 0, 1)\n",
    "\n",
    "\n",
    "##checking basic statistics of the dataset\n",
    "data.describe()\n",
    "\n",
    "## create a barona index score using all the variables before unnecessary variables are removed\n",
    "#first change the variables to the format needed for the index inlcuding sex and race\n",
    "data.loc[data['Sex'] == 'F' , 'Sex'] = 0\n",
    "data.loc[data['Sex'] == 'M' , 'Sex'] = 1\n",
    "\n",
    "data['Sex'] = data['Sex'].astype(int)\n",
    "\n",
    "#Replace data with strings based on condition\n",
    "data['Race'] = np.where(data['Race'] == 'White', 0, 1)\n",
    "\n",
    "#make the category colum an integer\n",
    "#data['Category'] = data['Category'].astype(int)\n",
    "#print(data.dtypes)\n",
    "\n",
    "# Define a function to calculate Barona Index\n",
    "def calculate_barona_index(row):\n",
    "    predicted_iq = 101.2 + (0.27 * row['Age']) + (0.54 * row['Education']) + (0.64 * row['Sex']) - (3.47 * row['Race'])\n",
    "    return predicted_iq\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "data['Barona_Index'] = data.apply(calculate_barona_index, axis=1)\n",
    "\n",
    "##move barona index to the front\n",
    "barona_insert = data.pop('Barona_Index')\n",
    "data.insert(2,'Barona_index',barona_insert)\n",
    "\n",
    "##removing non numeric and other columns\n",
    "data.drop(['DOB','MRN','Sex','Education','Race','Date (Neuro)','Visual Verbal Test','Letter Number Sequencing'],axis = 1 , inplace = True)\n",
    "\n",
    "#drop the unnamed column at the end of the dataset\n",
    "data.drop(data.columns[39], axis=1,inplace=True)\n",
    "\n",
    "##removing rows where there are no olfactory scores\n",
    "data = data.dropna(subset=['Threshold', 'Identification'], how='any')\n",
    "original_columns =pd.DataFrame(data.columns)\n",
    "original_columns =original_columns.T\n",
    "original_columns.columns = original_columns.iloc[0]  # Set first row as column names\n",
    "original_columns= original_columns[1:].reset_index(drop=True)  # Drop the first row and reset index\n",
    "\n",
    "\n",
    "#### we notice that there are a few cases where the olfactory scores are zero\n",
    "## we want to get rid of these particiants\n",
    "dropped_rows = data[data.Identification < 1]\n",
    "#print(dropped_rows)\n",
    "\n",
    "##dropping the 4 selected participants - ODIMA002_F1,0047,0053 and 1001_F1\n",
    "data.drop(data[data.Identification < 1].index, inplace=True)\n",
    "\n",
    "data_scaled = data.drop(data.columns[[0, 1]], axis=1)\n",
    "\n",
    "# Reset the index of the DataFrame as we have droppe some data earlier\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data_scaled.reset_index(drop=True, inplace=True)\n",
    "\n",
    "## create a new dataframe with the dropped columns so it can be concatanated\n",
    "dropped_columns = data.iloc[:, [0, 1]]\n",
    "\n",
    "#print(data_scaled.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataset with all variables standardized and one with all raw scores\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_scaled)  # Standardized data for clustering\n",
    "\n",
    "##taking the numpy array datascaled and making it a pandas dataframe\n",
    "data_scaled = pd.DataFrame(data_scaled)  # Convert to DataFrame if it's not already\n",
    "\n",
    "# Concatenate the dropped columns back to the front of the data_scaled DataFrame\n",
    "data_scaled = pd.concat([dropped_columns, data_scaled], axis=1)\n",
    "##renaming the dataframe columns with the string scaled so it can be used to create a master data table\n",
    "data_scaled.columns = [f\"{col}_scaled\" for col in original_columns]\n",
    "\n",
    "final_data = pd.concat([data, data_scaled.iloc[:,2:38]], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normality Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to test normality for multiple columns in the dataset and plot values to look at the distribution\n",
    "def check_normality(data):\n",
    "    results = []\n",
    "\n",
    "    for column in data.select_dtypes(include=[np.number]).columns:  # Select numeric columns only\n",
    "        print(f\"\\nüìä Checking Normality for ‚ûù {column}\")\n",
    "\n",
    "        # Histogram + KDE\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.histplot(data[column], kde=True, bins=30)\n",
    "        plt.title(f'Histogram & KDE for ‚ûù {column}')\n",
    "        plt.show()\n",
    "\n",
    "        # Q-Q Plot\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        stats.probplot(data[column].dropna(), dist=\"norm\", plot=plt)\n",
    "        plt.title(f'Q-Q Plot for ‚ûù {column}')\n",
    "        plt.show()\n",
    "\n",
    "        # Shapiro-Wilk Test\n",
    "        shapiro_stat, shapiro_p = stats.shapiro(data[column].dropna()) if len(data[column]) < 5000 else (None, None)\n",
    "\n",
    "        # Kolmogorov-Smirnov Test\n",
    "        ks_stat, ks_p = stats.kstest(data[column].dropna(), 'norm')\n",
    "\n",
    "        # Anderson-Darling Test\n",
    "        anderson_stat = stats.anderson(data[column].dropna(), dist='norm')\n",
    "\n",
    "        # Skewness & Kurtosis\n",
    "        skewness = stats.skew(data[column], nan_policy='omit')\n",
    "        kurt = stats.kurtosis(data[column], nan_policy='omit')\n",
    "\n",
    "        # Save results\n",
    "        results.append({\n",
    "            'Column': column,\n",
    "            'Shapiro P-Value': shapiro_p,\n",
    "            'KS P-Value': ks_p,\n",
    "            'Anderson Stat': anderson_stat.statistic,\n",
    "            'Skewness': skewness,\n",
    "            'Kurtosis': kurt\n",
    "        })\n",
    "\n",
    "        # Print results\n",
    "        print(f\"Shapiro-Wilk Test: p = {shapiro_p} {'‚úÖ Normal' if shapiro_p and shapiro_p > 0.05 else '‚ùå Not Normal'}\")\n",
    "        print(f\"KS Test: p = {ks_p} {'‚úÖ Normal' if ks_p > 0.05 else '‚ùå Not Normal'}\")\n",
    "        print(f\"Anderson-Darling Statistic: {anderson_stat.statistic}\")\n",
    "        print(f\"Skewness: {skewness}, Kurtosis: {kurt}\")\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run the function\n",
    "normality_results = check_normality(final_data)\n",
    "\n",
    "# Display summary results\n",
    "print(\"\\nüìå Summary of Normality Tests:\")\n",
    "print(normality_results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K- Means Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##Create a function to draw an elbow plot based on data to determine k\n",
    "max_k = 10 ## determine maximum k here \n",
    "##define the function below\n",
    "def optimise_k_means(data, max_k):\n",
    "    means = []\n",
    "    inertias = []\n",
    "\n",
    "    for k in range(1, max_k + 1):\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        kmeans.fit(data)\n",
    "\n",
    "        means.append(k)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "\n",
    "    return means, inertias\n",
    "\n",
    "\n",
    "\n",
    "##generate the data based on the above created function and maximum number of clusters\n",
    "\n",
    "means, inertias = optimise_k_means(data.iloc[:,2:38], max_k)\n",
    "\n",
    "## generate elbow plot\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(means, inertias, 'o-')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "## we will use either 2,3 and 4 cluster based on the elbow plot too see how the classification goes!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Kmeans cluster - All variables used in PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on the data to reduce dimensionality and allow clasifier algorithm to run\n",
    "## we use all the variables other than category to make the principal components\n",
    "pca = PCA(n_components=2)  # we set the PCA components to be 2 \n",
    "X_pca = pca.fit_transform(data_scaled.iloc[:,2:39])\n",
    "\n",
    "# Convert to the new PCA data into a data fram as it was a numpy array\n",
    "pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n",
    "\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "print(\"Cumulative Explained Variance:\", np.cumsum(pca.explained_variance_ratio_))\n",
    "\n",
    "##plot the principal components\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1],c=X_pca[:, 0], cmap='viridis', edgecolors='k', alpha=0.8)\n",
    "plt.colorbar(label=\"PC1 Value\")  # Adds a color legend\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.title(\"Principal Component Analyis Biplot - Standardized data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the PCA component loadings to understand which variables are the most important \n",
    "\n",
    "# Assuming PCA is already fitted\n",
    "loadings = pca.components_\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "loading_df = pd.DataFrame(loadings, columns=data_scaled.columns[2:39], index=['PC1', 'PC2'])\n",
    "#print(loading_df)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(loading_df, cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"PCA Loadings (Feature Contributions to PCs)\")\n",
    "plt.show()\n",
    "\n",
    "for i in range(2):  # Loop over PC1 and PC2\n",
    "    sorted_features = loading_df.iloc[i, :].abs().sort_values(ascending=False)\n",
    "    print(f\"Top 5 Features for {loading_df.index[i]}:\\n{sorted_features.head(5)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We run a k means cluster based to classify the raw data!\n",
    "#optimal_k = 3  # based on elbow diagram we set the number of clusters to be 3!\n",
    "#kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10) ## we set a random state for reproduceability!\n",
    "#clusters = kmeans.fit_predict(X_pca)\n",
    "\n",
    "# Add cluster labels to PCA DataFrame\n",
    "#pca_df['Cluster'] = clusters\n",
    "\n",
    "### plot principal compponents classified by clusters!!\n",
    "#plt.figure(figsize=(8,6))\n",
    "#plt.scatter(pca_df['PC1'], pca_df['PC2'], c=pca_df['Cluster'], cmap='viridis', alpha=1)\n",
    "#plt.xlabel(\"Principal Component 1\")\n",
    "#plt.ylabel(\"Principal Component 2\")\n",
    "#plt.title(f\"K-Means Clustering (k={optimal_k}) on PCA\")\n",
    "#plt.colorbar(label=\"Cluster\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We run a k means cluster based to classify the raw data!\n",
    "optimal_k = 3  # based on elbow diagram we set the number of clusters to be 3!\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10) ## we set a random state for reproduceability!\n",
    "clusters = kmeans.fit_predict(X_pca)\n",
    "\n",
    "# Add cluster labels to PCA DataFrame\n",
    "pca_df['Cluster'] = clusters\n",
    "\n",
    "##add the ODIMA_ID and original classification to the data frame so i can plot \n",
    "ODIMA_CAT_Master = data_scaled[['Study ID_scaled','Category_scaled']]  #excract columns from DataFrame\n",
    "\n",
    "# Rename the columns to the original names\n",
    "ODIMA_CAT_Master.columns = ['Study ID', 'Category']\n",
    "\n",
    "# Insert the columns into pca_df at the desired position\n",
    "pca_df.insert(2, 'Study ID', ODIMA_CAT_Master['Study ID'])\n",
    "pca_df.insert(3, 'Category', ODIMA_CAT_Master['Category'])\n",
    "# Ensure Category is numeric or mapped to numbers\n",
    "pca_df['Category'] = pca_df['Category'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## basic plot which is no longer in use due to a color cordinated one being used\n",
    "\n",
    "# Create scatter plot\n",
    "#plt.figure(figsize=(8, 6))\n",
    "#plt.scatter(pca_df['PC1'], pca_df['PC2'], \n",
    "#c=pca_df['Cluster'], cmap= 'viridis', alpha=1)\n",
    "# Add labels and title\n",
    "#plt.xlabel(\"Principal Component 1\")\n",
    "#plt.ylabel(\"Principal Component 2\")\n",
    "#plt.title(f\"K-Means Clustering (k={3}) on PCA\")  # Update optimal_k accordingly\n",
    "#plt.colorbar(label=\"Cluster\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a basic plot with colors assigned to cluster for easier recognition\n",
    "# Custom colors for each cluster are assigned\n",
    "cluster_colors = {0: 'green', 1: 'red', 2: 'yellow'}\n",
    "# names are assigned to each cluster\n",
    "cluster_names = {0: 'Cluster 0 - Mainly HC', 1: 'Cluster 1 - Mainly MCI', 2: 'Cluster 2 - Mixed bag'}\n",
    "\n",
    "# Create scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "for category in np.unique(pca_df['Cluster']):\n",
    "    category_data_1 = pca_df[pca_df['Cluster'] == category]\n",
    "    plt.scatter(category_data_1['PC1'], category_data_1['PC2'], \n",
    "                color=cluster_colors[category], label=cluster_names.get(category, f'Category {category}'), alpha=0.7)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.title(\"PCA - K Means=3 Classifier\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotly graph for the above so that it is interactive\n",
    "# Set the default renderer to open up in a browser\n",
    "pio.renderers.default = \"browser\"\n",
    "\n",
    "# Custom colors for each Cluster\n",
    "cluster_colors = {0: 'green', 1: 'red', 2: 'yellow'}\n",
    "# Mapping Cluster numbers to new names\n",
    "cluster_names_n = {0: 'Cluster 0- Mainly HC', 1: 'Cluster 1- Mainly MCI', 2: 'Cluster 2- Mixed bag'}\n",
    "\n",
    "# Ensure 'Cluster' is treated as a categorical variable for the plots\n",
    "pca_df['Cluster'] = pca_df['Cluster'].astype('category')\n",
    "\n",
    "# Set a template for the plot\n",
    "pio.templates.default = \"simple_white\"  # Replace as needed\n",
    "\n",
    "# Create an interactive scatter plot with Plotly\n",
    "fig = px.scatter(\n",
    "    pca_df, \n",
    "    x='PC1', \n",
    "    y='PC2', \n",
    "    color='Cluster', \n",
    "    hover_data=['Study ID'],  # Display Study ID on hover to see which participants need to be focused on\n",
    "    title=\"PCA - New Kmeans 3 Classifier\",\n",
    "    labels={\n",
    "        'PC1': 'Principal Component 1',\n",
    "        'PC2': 'Principal Component 2',\n",
    "        'Category': 'New Clusters'\n",
    "    },\n",
    "    color_discrete_map=cluster_colors  # Set custom colors for categories\n",
    ")\n",
    "\n",
    "# Update legend labels using the cluster_names_n dictionary\n",
    "fig.for_each_trace(lambda t: t.update(name=cluster_names_n.get(int(t.name), t.name)))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Principal Components based on their original Group in the ODIMA study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot principal compponents classified by the original ODIMA study group\n",
    "#no longer in use as new plot is made with consistent color coding\n",
    "\n",
    "#plt.figure(figsize=(8,6))\n",
    "#plt.scatter(pca_df['PC1'], pca_df['PC2'], c=pca_df['Category'], cmap='viridis', alpha=1)\n",
    "\n",
    "\n",
    "# Add text labels for each point using Study ID ## removed as it makes the plot impossible to see\n",
    "#for i, txt in enumerate(pca_df['Study ID']):\n",
    "    #plt.annotate(txt, (pca_df['PC1'].iloc[i], pca_df['PC2'].iloc[i]), \n",
    "                 #fontsize=8, alpha=0.7)\n",
    "\n",
    "#plt.xlabel(\"Principal Component 1\")\n",
    "#plt.ylabel(\"Principal Component 2\")\n",
    "#plt.title(\"PCA - Original ODIMA Category Classification\")\n",
    "#plt.colorbar(label=\"ODIMA Category\")\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom colors for each ODIMA category\n",
    "category_colors = {0: 'Green', 1: 'red'}\n",
    "#name the columns\n",
    "category_names = {0: 'Healthy Controls', 1: 'Mild Cognitive Impairment'}\n",
    "\n",
    "# Create scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "for category in np.unique(pca_df['Category']):\n",
    "    category_data = pca_df[pca_df['Category'] == category]\n",
    "    plt.scatter(category_data['PC1'], category_data['PC2'], \n",
    "                color=category_colors[category], label=category_names.get(category, f'Category {category}'), alpha=0.7)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.title(\"PCA - Original ODIMA Category Classification\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotly graph for the above so that it is interactive , colors have been set to be consistent\n",
    "\n",
    "# Set the default renderer\n",
    "pio.renderers.default = \"browser\"\n",
    "\n",
    "# Custom colors for Healthy Controls (0) and MCI (1)\n",
    "category_colors_n = {0: 'green', 1: 'red'}\n",
    "# Mapping category numbers to new names\n",
    "category_names_n = {0: 'Healthy Controls', 1: 'Mild Cognitive Impairment'}\n",
    "# Ensure 'Category' is treated as a categorical variable\n",
    "pca_df['Category'] = pca_df['Category'].astype('category')\n",
    "\n",
    "# Set the default template to a built-in theme\n",
    "pio.templates.default = \"simple_white\"  # Replace as needed\n",
    "\n",
    "# Create an interactive scatter plot with Plotly\n",
    "fig = px.scatter(\n",
    "    pca_df, \n",
    "    x='PC1', \n",
    "    y='PC2', \n",
    "    color='Category', \n",
    "    hover_data=['Study ID'],  # Display Study ID on hover\n",
    "    title=\"PCA - Original ODIMA Category Classification\",\n",
    "    labels={\n",
    "        'PC1': 'Principal Component 1',\n",
    "        'PC2': 'Principal Component 2',\n",
    "        'Category': 'ODIMA Category'\n",
    "    },\n",
    "    color_discrete_map=category_colors_n  # Set custom colors for categories\n",
    ")\n",
    "# Update legend labels using the category_names dictionary\n",
    "fig.for_each_trace(lambda t: t.update(name=category_names_n.get(int(t.name), t.name)))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##next i take the new clusters and then add them back to the original dataset for further analysis!\n",
    "#print(len(data))\n",
    "#print(len(final_data))\n",
    "#print(len(pca_df))\n",
    "#print(pca_df['Cluster'].isna().sum()) \n",
    "\n",
    "##creating a new data frame with the cluster information for analaysis\n",
    "#dataset is named based on number of kmeans clustering used\n",
    "cluster3 = pd.concat([data , pd.DataFrame({'Cluster': pca_df['Cluster'].values})], axis=1)\n",
    "\n",
    "##next i want to move the cluster columns to where the initial subject category column is to see how the data has been classified and make analysis easier!\n",
    " \n",
    "# Move column 'Cluster' to be the first column\n",
    "col = cluster3.pop('Cluster')  # Remove 'C' from DataFrame\n",
    "cluster3.insert(2, 'Cluster', col)  # Insert 'C' at index 0\n",
    "\n",
    "## CLuster 0\n",
    "df_filtered_3_0 = cluster3[cluster3['Cluster'] == 0]\n",
    "#print(df_filtered_3_0)\n",
    "\n",
    "## count the number of zero cluster by category\n",
    "result_3_0 = df_filtered_3_0.groupby('Category')['Cluster'].count()\n",
    "result_3_0 = result_3_0.reset_index(name='Count')\n",
    "\n",
    "##creating a perecntage to see how many HC and MCI are being classified as cluster 0\n",
    "result_3_0['Percentage']= (result_3_0['Count'] / result_3_0['Count'].sum())*100\n",
    "print(result_3_0)\n",
    "\n",
    "\n",
    "## Cluster 1\n",
    "df_filtered_3_1 = cluster3[cluster3['Cluster'] == 1]\n",
    "#print(df_filtered_3_1)\n",
    "\n",
    "## count the number of zero cluster by category\n",
    "result_3_1= df_filtered_3_1.groupby('Category')['Cluster'].count()\n",
    "result_3_1 = result_3_1.reset_index(name='Count')\n",
    "\n",
    "##creating a perecntage to see how many HC and MCI are being classified as cluster 0\n",
    "result_3_1['Percentage']= (result_3_1['Count'] / result_3_1['Count'].sum())*100\n",
    "print(result_3_1)\n",
    "\n",
    "###Cluster 2\n",
    "df_filtered_3_2 = cluster3[cluster3['Cluster'] == 2]\n",
    "#print(df_filtered_3_2)\n",
    "\n",
    "## count the number of zero cluster by category\n",
    "result_3_2= df_filtered_3_2.groupby('Category')['Cluster'].count()\n",
    "result_3_2 = result_3_2.reset_index(name='Count')\n",
    "\n",
    "##creating a perecntage to see how many HC and MCI are being classified as cluster 0\n",
    "result_3_2['Percentage']= (result_3_2['Count'] / result_3_2['Count'].sum())*100\n",
    "print(result_3_2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
